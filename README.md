# ğŸ§  Transformer-Based Text Summarization (TensorFlow)

An end-to-end implementation of an **abstractive text summarization model**
using a custom **Transformer (Encoderâ€“Decoder)** architecture built with
TensorFlow and Keras.

--------------------------------------------------------------------------

## âœ¨ Features

- Custom Transformer implementation (no high-level shortcuts)
- Encoderâ€“Decoder architecture with Multi-Head Attention
- Look-ahead and padding masks
- Teacher forcing during training
- Greedy decoding during inference
- SOS / EOS token-based sequence generation
- Fully reproducible training pipeline

--------------------------------------------------------------------------

## ğŸ“ Project Structure

transformer_model/
â”‚
â”œâ”€â”€ main.py                # Training & inference pipeline
â”œâ”€â”€ Transformer.py         # Full Transformer model
â”œâ”€â”€ Encoder.py             # Encoder stack
â”œâ”€â”€ Decoder.py             # Decoder stack
â”œâ”€â”€ DecoderLayer.py        # Masked attention decoder layer
â”œâ”€â”€ helper.py              # Masks, preprocessing, utilities
â”œâ”€â”€ corpus/                # Training & test datasets
â”‚   â”œâ”€â”€ train.json
â”‚   â””â”€â”€ test.json
â”œâ”€â”€ requirements.txt       # Dependencies (pip freeze)
â””â”€â”€ README.md

--------------------------------------------------------------------------

## ğŸ— Model Architecture

### Encoder
- Token embedding + positional encoding
- Stacked encoder layers
- Multi-head self-attention
- Feed-forward networks

### Decoder
- Masked self-attention (look-ahead mask)
- Encoderâ€“decoder attention (padding mask)
- Feed-forward network
- Final softmax over vocabulary

--------------------------------------------------------------------------

## âš™ Training Configuration

### Sequence Lengths
- Encoder max length: 150
- Decoder max length: 50

### Hyperparameters
- Embedding dimension: 128
- Number of layers: 2
- Attention heads: 2
- Batch size: 64
- Epochs: 20

### Optimization
- Optimizer: Adam
- Learning rate: Custom warmup schedule
- Loss: Masked Sparse Categorical Crossentropy

--------------------------------------------------------------------------

## ğŸ“‰ Loss Function

- Padding tokens are ignored
- Loss is computed only on valid tokens
- Normalized by number of non-padding tokens

--------------------------------------------------------------------------

## ğŸ” Inference & Summarization

### How Inference Works
1. Encode the input document
2. Initialize decoder with [SOS]
3. Predict tokens step-by-step
4. Stop at [EOS] or max decoder length

### Example

Input:
[SOS] amanda: i baked cookies... [EOS]

Human Summary:
[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]

Model Output:
Generated using greedy decoding

--------------------------------------------------------------------------

## â–¶ How to Run

### 1ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

### 2ï¸âƒ£ Prepare Dataset

Place your dataset inside:

./corpus/

### 3ï¸âƒ£ Train the Model

python main.py

During training, the script will:
- Print batch-wise progress
- Report loss after each epoch
- Show example summarizations from the test set

--------------------------------------------------------------------------

## ğŸ’¾ Saving & Loading the Model

Save model weights:

transformer.save_weights("transformer_weights.h5")

Load model weights:

transformer.load_weights("transformer_weights.h5")

Once loaded, you can directly generate summaries:

summarize(transformer, input_text)

--------------------------------------------------------------------------

## ğŸ“Œ Notes

- The model does NOT need retraining if weights are saved and reloaded
- Inference runs with training=False
- Designed for clarity and educational purposes
